import torch
import random
import torch.nn.functional as F
from spikingjelly.activation_based import functional, surrogate, neuron
import time
import os
import matplotlib.pyplot as plt
import numpy as np

def evaluate(net, test_data_loader, device, train_time):
    net.eval()
    test_loss = 0
    test_acc = 0
    test_samples = 0

    with torch.no_grad():
        for frame, label in test_data_loader:
            frame = frame.to(device)
            frame = frame.transpose(0, 1)
            label = label.to(device)
            label_onehot = F.one_hot(label, 11).float()

            output_with_time, batch_spikes,fsr_per_neuron = net(frame)
            out_fr = output_with_time.mean(0)
            loss = F.mse_loss(out_fr, label_onehot)

            test_samples += label.numel()
            test_loss += loss.item() * label.numel()
            test_acc += (out_fr.argmax(1) == label).float().sum().item()

            functional.reset_net(net)

    test_time = time.time()
    test_speed = test_samples / (test_time - train_time)
    test_loss /= test_samples
    test_acc /= test_samples

    return test_loss, test_acc, test_speed

def compute_fisher_information(net, data_loader, device):
    net.train()
    total_fisher = None
    sample_count = 0

    for frame, label in data_loader:
        frame = frame.to(device).transpose(0, 1)
        label = label.to(device)
        label_onehot = F.one_hot(label, num_classes=11).float()

        net.zero_grad()

        output_with_time, _, _ = net(frame)  # neuron_output ä¸å†éœ€è¦
        out_fr = output_with_time.mean(0)
        loss = F.mse_loss(out_fr, label_onehot)
        loss.backward()

        with torch.no_grad():
            param = net.fc.weight
            if param.grad is not None:
                grad_sq = param.grad.pow(2)
                if total_fisher is None:
                    total_fisher = grad_sq.clone()
                else:
                    total_fisher += grad_sq

        sample_count += frame.shape[1]  # batch size
        functional.reset_net(net)

    fisher_info = total_fisher / sample_count #fisher info torch.Size([512, 1024])

    U, S, Vh = torch.linalg.svd(fisher_info, full_matrices=False)

    # æœ€å¤§å¥‡å¼‚å€¼å¯¹åº”çš„å·¦å¥‡å¼‚å‘é‡ï¼ˆshape: [512]ï¼‰
    dominant_vector = abs(Vh[:, 0])

    # dominant_vector = dominant_vector / dominant_vector.norm()

    return dominant_vector

# ****************
    # # å‡è®¾ fisher_info æ˜¯ shape [512, 1024] çš„ torch.Tensor
    # A = fisher_info
    # AAT = A @ A.T  # shape: [512, 512]ï¼Œç­‰ä»·äº A A^T
    #
    # # è®¡ç®— AAT çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    # eigenvalues, eigenvectors = torch.linalg.eigh(AAT)  # é€‚ç”¨äºå¯¹ç§°çŸ©é˜µ
    #
    # # æœ€å¤§ç‰¹å¾å€¼åŠå…¶å¯¹åº”çš„ç‰¹å¾å‘é‡
    # max_eigenvalue, max_index = torch.max(eigenvalues, dim=0)
    # dominant_eigenvector = eigenvectors[:, max_index]  # shape: [512]

#****************
    # print('fisher info', fisher_info.shape)
    # fisher_info_per_input = fisher_info.sum(dim=1)  # shape: [512]
    # fisher_info_per_input = fisher_info_per_input * net.mask


# with torch.no_grad():
#     # è®¡ç®— Fisher ä¿¡æ¯ï¼ˆå¹³å‡æ¯ä¸ªæ ·æœ¬çš„å¹³æ–¹æ¢¯åº¦ï¼‰
#     batch_fisher = neuron_output.grad.pow(2).sum(dim=(0, 1))  # sum over T and B
#     if total_fisher is None:
#         total_fisher = batch_fisher
#     else:
#         total_fisher += batch_fisher
#
# sample_count += neuron_output.shape[0] * neuron_output.shape[1]
# functional.reset_net(net)  # é‡ç½®è†œç”µä½ç­‰çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæ ·æœ¬




def spearman_corr(x, y):
    # å°†ä¸¤ä¸ªå‘é‡è½¬æ¢ä¸ºæ’å
    x_rank = np.argsort(np.argsort(x))
    y_rank = np.argsort(np.argsort(y))

    # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
    return np.corrcoef(x_rank, y_rank)[0, 1]
def set_seed(seed):
    random.seed(seed)  # Python å†…ç½®éšæœºæ•°
    np.random.seed(seed)  # NumPy éšæœºæ•°
    torch.manual_seed(seed)  # PyTorch CPU éšæœºæ•°
    torch.cuda.manual_seed(seed)  # PyTorch GPU éšæœºæ•°ï¼ˆå•å¡ï¼‰
    torch.cuda.manual_seed_all(seed)  # PyTorch GPU éšæœºæ•°ï¼ˆå¤šå¡ï¼‰

    # cuDNN ä¸­ä¸€äº›æ“ä½œæ˜¯éç¡®å®šæ€§çš„ï¼Œè¿™é‡Œå¼ºåˆ¶å®ƒä¸ºç¡®å®šæ€§æ¨¡å¼
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def save_checkpoint(model, optimizer, epoch, loss, scheduler=None, dir_path='checkpoints/',train_acc_list=None,
                    test_acc_list=None, epoch_list=None,spearman_list=None,pearson_list=None,pruning_started=None, prune_stopped=None,
                    first_prune_epoch=None, pruning_count=None,
                    prune_check=None, neuron_acc_history=None,prune_epoch=None, num_retained=None):
    os.makedirs(dir_path, exist_ok=True)

    filename = f'checkpoint_epoch_{epoch}.pth'
    path = os.path.join(dir_path, filename)

    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss
    }
    if scheduler:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()

    if train_acc_list is not None and test_acc_list is not None and epoch_list is not None and spearman_list is not None and pearson_list is not None and neuron_acc_history is not None and prune_epoch is not None:
        checkpoint['train_acc_list'] = train_acc_list
        checkpoint['test_acc_list'] = test_acc_list
        checkpoint['epoch_list'] = epoch_list
        checkpoint['spearman_list'] = spearman_list
        checkpoint['pearson_list'] = pearson_list
        checkpoint['neuron_acc_history'] = neuron_acc_history
        checkpoint['prune_epoch'] = prune_epoch

    if num_retained is not None:
        checkpoint['num_retained'] = num_retained

    checkpoint['pruning_started'] = pruning_started
    checkpoint['prune_stopped'] = prune_stopped
    checkpoint['first_prune_epoch'] = first_prune_epoch
    checkpoint['pruning_count'] = pruning_count
    checkpoint['prune_check'] = prune_check
    checkpoint['mask'] = model.mask.cpu()  # å»ºè®®ä¿å­˜ä¸º cpu ç‰ˆæœ¬

    torch.save(checkpoint, path)
    print(f'Checkpoint saved to {path}')



def load_checkpoint(model, optimizer, scheduler=None, path='checkpoints/checkpoint_epoch_150.pth'):
    checkpoint = torch.load(path, weights_only=False)

    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    epoch = checkpoint['epoch']
    loss = checkpoint['loss']

    if scheduler and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    train_acc_list = checkpoint.get('train_acc_list', [])
    test_acc_list = checkpoint.get('test_acc_list', [])
    epoch_list = checkpoint.get('epoch_list', [])
    spearman_list = checkpoint.get('spearman_list', [])
    pearson_list = checkpoint.get('pearson_list', [])

    pruning_started = checkpoint.get('pruning_started', False)
    prune_stopped = checkpoint.get('prune_stopped', False)
    first_prune_epoch = checkpoint.get('first_prune_epoch', None)
    pruning_count = checkpoint.get('pruning_count', 0)
    prune_check = checkpoint.get('prune_check', 0)
    neuron_acc_history = checkpoint.get('neuron_acc_history', [])
    prune_epoch = checkpoint.get('prune_epoch', [])
    num_retained=checkpoint.get('num_retained', 512)
    if 'mask' in checkpoint:
        model.mask = checkpoint['mask'].to(model.device)  # âœ… æ¢å¤ mask

    print(f'Checkpoint loaded from {path}, resuming from epoch {epoch}')
    return epoch, loss ,train_acc_list, test_acc_list, epoch_list, spearman_list, pearson_list,pruning_started, prune_stopped, first_prune_epoch,pruning_count, prune_check, neuron_acc_history,prune_epoch, num_retained


def plot_fisher_vs_fsr(fisher_np, fsr_np, pruned_indices, epoch, pruning_count, save_dir):
    """
    ç»˜åˆ¶æ¯è½®å‰ªæåç¥ç»å…ƒçš„ FSR ä¸ Fisher ä¿¡æ¯æŸ±çŠ¶å›¾ï¼ˆå‰”é™¤å€¼ä¸º0çš„ç¥ç»å…ƒï¼Œé‡æ’æ¨ªè½´ï¼‰

    å‚æ•°ï¼š
    - fisher_np: np.array[N]ï¼Œæ¯ä¸ªç¥ç»å…ƒçš„ Fisher ä¿¡æ¯
    - fsr_np: np.array[N]ï¼Œæ¯ä¸ªç¥ç»å…ƒçš„ FSR
    - pruned_indices: list æˆ– Tensorï¼Œè¢«å‰ªæ‰çš„ç¥ç»å…ƒç´¢å¼•
    - epoch: å½“å‰å‰ªæå‘ç”Ÿçš„ epoch ç¼–å·
    - pruning_count: å½“å‰å‰ªæè½®æ•°
    - save_dir: å›¾åƒä¿å­˜ç›®å½•
    """
    assert fisher_np.shape == fsr_np.shape, "Fisher å’Œ FSR å½¢çŠ¶å¿…é¡»ä¸€è‡´"

    # æ©ç ï¼šåŒæ—¶å‰”é™¤ fsr å’Œ fisher éƒ½ä¸º 0 çš„ç¥ç»å…ƒ
    nonzero_mask = ~(fsr_np == 0)
    filtered_fsr = fsr_np[nonzero_mask]
    filtered_fisher = fisher_np[nonzero_mask]

    # ğŸŸ¢ æ¨ªè½´ç¼–å·é‡æ’ä¸º 0, 1, ..., N'-1
    filtered_indices = np.arange(len(filtered_fsr))

    fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)
    width = 0.6

    # ç¬¬ä¸€å¼ å­å›¾ï¼šFSR
    axes[0].bar(filtered_indices, filtered_fsr, width=width, label='FSR', color='skyblue')
    axes[0].set_ylabel("FSR Value")
    axes[0].set_title(f"FSR (Epoch {epoch}, # {pruning_count})")
    axes[0].legend()
    axes[0].grid(True, linestyle='--', alpha=0.4)

    # ç¬¬äºŒå¼ å­å›¾ï¼šFisher Info
    axes[1].bar(filtered_indices, filtered_fisher, width=width, label='Fisher Info', color='orange')
    axes[1].set_xlabel("Neuron Index")
    axes[1].set_ylabel("Fisher Value")
    axes[1].set_title(f"Fisher Information (Epoch {epoch}, # {pruning_count})")
    axes[1].legend()
    axes[1].grid(True, linestyle='--', alpha=0.4)

    plt.tight_layout()

    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, f"fisher_vs_fsr_epoch_{epoch}_prune_{pruning_count}.png")
    plt.savefig(save_path)
    plt.close()



# def plot_fisher_vs_fsr(fisher_np, fsr_np, pruned_indices, epoch, pruning_count, save_dir):
#     """
#     ç»˜åˆ¶æ¯è½®å‰ªæåç¥ç»å…ƒçš„ FSR ä¸ Fisher ä¿¡æ¯æŸ±çŠ¶å›¾
#
#     å‚æ•°ï¼š
#     - fisher_info: Tensor[N]ï¼Œæ¯ä¸ªç¥ç»å…ƒçš„ Fisher ä¿¡æ¯
#     - fsr_per_neuron: Tensor[N]ï¼Œæ¯ä¸ªç¥ç»å…ƒçš„ FSR
#     - pruned_indices: list æˆ– Tensorï¼Œè¢«å‰ªæ‰çš„ç¥ç»å…ƒç´¢å¼•
#     - epoch: å½“å‰å‰ªæå‘ç”Ÿçš„ epoch ç¼–å·
#     - save_dir: å›¾åƒä¿å­˜ç›®å½•
#     """
#     indices = np.arange(len(fsr_np))
#
#     fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)
#
#     width = 0.6  # å­å›¾ä¸­å¯ä»¥ç¨å¾®åŠ å®½æŸ±çŠ¶å›¾çš„å®½åº¦
#
#     # ç¬¬ä¸€å¼ å­å›¾ï¼šFSR
#     axes[0].bar(indices, fsr_np, width=width, label='FSR', color='skyblue')
#     axes[0].set_ylabel("FSR Value")
#     axes[0].set_title(f"FSR (Epoch {epoch}, # {pruning_count})")
#     axes[0].legend()
#     axes[0].grid(True, linestyle='--', alpha=0.4)
#
#     # # çº¢è‰²è™šçº¿æ ‡å‡ºå‰ªæ‰çš„ç¥ç»å…ƒ
#     # if pruned_indices is not None and len(pruned_indices) > 0:
#     #     for idx in pruned_indices.cpu().tolist():
#     #         axes[0].axvline(x=idx, color='red', linestyle='--', linewidth=0.6, alpha=0.6)
#
#     # ç¬¬äºŒå¼ å­å›¾ï¼šFisher Info
#     axes[1].bar(indices, fisher_np, width=width, label='Fisher Info', color='orange')
#     axes[1].set_xlabel("Neuron Index")
#     axes[1].set_ylabel("Fisher Value")
#     axes[1].set_title(f"Fisher Information (Epoch {epoch}, # {pruning_count})")
#     axes[1].legend()
#     axes[1].grid(True, linestyle='--', alpha=0.4)
#     #
#     # # çº¢è‰²è™šçº¿æ ‡å‡ºå‰ªæ‰çš„ç¥ç»å…ƒ
#     # if pruned_indices is not None and len(pruned_indices) > 0:
#     #     for idx in pruned_indices.cpu().tolist():
#     #         axes[1].axvline(x=idx, color='red', linestyle='--', linewidth=0.6, alpha=0.6)
#
#     plt.tight_layout()
#
#     os.makedirs(save_dir, exist_ok=True)
#     save_path = os.path.join(save_dir, f"fisher_vs_fsr_epoch_{epoch}_prune_{pruning_count}.png")
#     plt.savefig(save_path)
#     plt.close()

def log_fisher_fsr_statistics(fisher_info, fsr_per_neuron, pruned_indices,
                               epoch, pruning_count, save_dir,
                               spearman_list, pearson_list):
    """
    è®¡ç®— FSR ä¸ Fisher ä¿¡æ¯çš„ç›¸å…³æ€§ã€å¯è§†åŒ–ï¼Œå¹¶è®°å½•è¿›åˆ—è¡¨ä¸­ã€‚

    å‚æ•°:
    - fisher_info: torch.Tensor, Fisher ä¿¡æ¯
    - fsr_per_neuron: torch.Tensor, FSR ä¿¡æ¯
    - pruned_indices: torch.Tensor æˆ– list, è¢«å‰ªæ‰çš„ç¥ç»å…ƒç´¢å¼•
    - epoch: å½“å‰ epoch
    - pruning_count: å½“å‰æ˜¯ç¬¬å‡ æ¬¡å‰ªæ
    - save_dir: ä¿å­˜å›¾åƒçš„è·¯å¾„
    - spearman_list: ç”¨äºä¿å­˜ spearman ç›¸å…³ç³»æ•°çš„åˆ—è¡¨ï¼ˆä¼ å¼•ç”¨ï¼‰
    - pearson_list: ç”¨äºä¿å­˜ pearson ç›¸å…³ç³»æ•°çš„åˆ—è¡¨ï¼ˆä¼ å¼•ç”¨ï¼‰
    """
    fisher_np = fisher_info.detach().cpu().numpy()
    fsr_np = fsr_per_neuron.detach().cpu().numpy()

    # å¯è§†åŒ–
    plot_fisher_vs_fsr(fisher_np, fsr_np, pruned_indices, epoch, pruning_count, save_dir=save_dir)

    # è®¡ç®—ç›¸å…³æ€§
    spearman_corr_val = spearman_corr(fsr_np, fisher_np)
    pearson_corr_val = np.corrcoef(fsr_np, fisher_np)[0, 1]

    # åŠ å…¥åˆ—è¡¨ä¸­
    spearman_list.append(spearman_corr_val)
    pearson_list.append(pearson_corr_val)


def record_pruning_info(epoch, pruning_count, num_pruned, num_retained,
                        last_prune_acc, test_loss, test_acc, test_speed,
                        neuron_acc_history, prune_epoch):
    """
    è®°å½•å½“å‰å‰ªæåçš„ä¿¡æ¯å¹¶æ›´æ–°æ—¥å¿—åˆ—è¡¨ã€‚

    å‚æ•°:
    - epoch: å½“å‰ epoch
    - pruning_count: å½“å‰ç¬¬å‡ æ¬¡å‰ªæ
    - num_pruned: å‰ªæ‰çš„ç¥ç»å…ƒæ•°é‡
    - num_retained: å‰©ä½™ç¥ç»å…ƒæ•°é‡
    - last_prune_acc: å‰ªæå‰ä¸€è½®çš„å‡†ç¡®ç‡ï¼ˆå¯é€‰ï¼‰
    - test_loss, test_acc, test_speed: å½“å‰å‰ªæåçš„æµ‹è¯•æŒ‡æ ‡
    - neuron_acc_history: ç²¾åº¦-å‰ªæè®°å½•åˆ—è¡¨ï¼ˆå¼•ç”¨ä¼ å…¥ï¼‰
    - prune_epoch: å‰ªæå‘ç”Ÿçš„ epoch åˆ—è¡¨ï¼ˆå¼•ç”¨ä¼ å…¥ï¼‰

    è¿”å›:
    - pruning_count: å‰ªææ¬¡æ•° +1 åçš„å€¼
    - last_prune_acc: é‡ç½®ä¸º None
    """
    prune_epoch.append(epoch)
    pruning_count += 1

    # è®°å½•å†å²å‰ªæä¿¡æ¯
    neuron_acc_history.append(
        (num_pruned, last_prune_acc if last_prune_acc is not None else test_acc, num_retained)
    )

    print(f"\n--- ç¬¬ {pruning_count} æ¬¡å‰ªæå·²æ‰§è¡Œ (Epoch: {epoch}) ---")
    print(f" å‰ªæ‰ç¥ç»å…ƒæ•°é‡: {num_pruned}")
    # print(f" å‰ªæåå‰©ä½™ç¥ç»å…ƒæ•°é‡: {num_retained}")

    return pruning_count, None  # last_prune_acc é‡ç½®ä¸º None

